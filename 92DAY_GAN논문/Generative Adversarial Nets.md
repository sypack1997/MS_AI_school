## Abstract

GAN의 모델은 다음과 같이 구성되어 있다.

- **generator G : 입력정보를 바탕으로 가상의 결과물을 만들어내는 모델**
- **discriminator D : G가 아닌 훈련데이터에서 샘플이 나왔을 확률을 추정하는 모델**

즉, 모델 G의 훈련과정은 D가 실수할 확률을 최대화한다. G는 훈련 데이터의 분포를 학습하여, 임의의 노이즈를 입력 받아 훈련 데이터와 같은 분포로 생성하고, D는 해당 인풋이 생성된 이미지인지 훈련 데이터로부터 나온 이미지인지에 대한 확률이 1/2가 되게 한다.

## 1. Introduction

쉽게 이해를 위해 위조지폐범과 경찰을 예시로 하여 설명하는 글이 많다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0a1b4191-f5cb-49ef-9b15-f445133d187d/Untitled.png)

adversarial nets 프레임워크에서 generator(위조지폐범) 모델은 discriminator(경찰) 모델을 속이도록 세팅되고 discriminator 모델은 샘플이 G가 모델링한 분포에서 나온것인지 실제 데이터 분포에서 나온것이지 결정하는 법을 학습한다. 이러한 경쟁구도를 통해 두 모델이 각각의 목적을 달성시키기 위해 스스로를 개선하도록 한다.

G모델은 다층 퍼셉트론으로 구성되어 random noise를 전달하여 데이터를 생성한다. 또한 D모델도 다층 퍼셉트론으로 구성된다. 이 구조를 adversarial net이라고 한다.

두 모델은 역전파와 드롭아웃 알고리즘으로 학습할 수 있으며 G 모델로부터 나오는 샘플은 순전파로 생성된다.

## 2. Adversarial nets

모델이 둘 다 다층 퍼셉트론일때 간단히 적용할 수 있다.

자세한 학습과정에 대한 수식은 생략하겠다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d9df0145-77d9-4810-90d3-7259954435d2/Untitled.png)

- 첫번째 항 : 실제 데이터 x를 D모델에 넣었을 때 나오는 결과(x가 실제 데이터 분포에서 나왔을 확률)에 log를 취해 얻는 기댓값
- 두번째 항 : fake데이터 z를 G모델에 넣은 결과를 D 모델에 넣었을때 결과에 log(1-결과)를 취해 얻는 기댓값

**D의 입장**에서 G가 생성한 가짜 데이터가 들어오면 0을 출력하고 실제 데이터가 들어오면 1을 출력해야한다. 따라서 목적함수의 각 항이 0의 값이 되도록 만들어야하고, **V(D,G)를 최대화하는 값은 0이 된다.**

**G의 입장**에서는 D가 샘플이 실제 데이터 분포에서 나온 것으로 판단하게 만들어야 하기 때문에 D(G(z))의 값이 1이 되도록 만들어야 한다. 즉, **V(D,G)가 음의 무한대 값(log0)으로 가도록 만들어야 한다.**

학습시키는 과정에서 inner loop에서 D를 최적화하는 것은 계산량이 많고 유한한 데이터 셋에서는 과적합을 초래한다. **따라서, D의 가중치 계산을 줄이기 위해 D를 최적화하는 k step과 G를 최적화 하는 1step을 번갈아 수행한다. 이를 통해 D는 최적의 솔루션에 가깝게 유지가 되고,  G또한 충분히 천천히 변화했다.**

학습의 진행과정은 다음과 같다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2a1e8c7b-0539-463b-9bcb-caca164689ae/Untitled.png)

- D가 모델링하는 조건부 확률분포 : 파란점선
- G가 모델링하는 생성분포 : 녹색실선
- 실제데이터 생성분포 : 검은점선
- 화살표 : 매핑을 통과한 샘플들이 어떤식으로 non-uniform한지를 나타냄

(a) 학습 초기의 분포 상태

(b) D의 분포가 분명하게 데이터를 판별 중

(c) 어느 정도 D의 학습이 이루어지면, G는 실제 데이터의 분포를 모사하여 D가 판별하기 힘들게 학습한다.

(d) 이 과정을 반복하여 실제 데이터분포와 G에 의해 생성된 분포가 거의 비슷해져 D는 1/2의 값에 가까운 확률을 보여준다.

## 3. Advantages and disadvantages

### 3.1 Disadvantages

D와 G가 균형을 잘 맞춰 성능이 향상되어야 한다.

### 3.2 Advantages

- 역전파로만 훈련이 가능하다.
- generator network가 실제 데이터로부터 직접적으로 업데이트되지 않고 discriminator로부터 흘러들어오는 gradient만을 이용해 학습될 수 있어 통계적 이점을 가져온다.
- 학승 중 inference가 필요하지 않다.
- 다양한 함수들이 모델에 접목될 수 있다.
- Markov chains를 기반으로 하는 방법보다 선명한 이미지를 얻을 수 있다.

## 4. Cinclusions and future work

- 클래스 레이블을 추가하여 Conditional generative model(조건부 생성 모델)을 얻을 수 있다.
- 준지도학습 : D에 의해 얻어지는 중간단계 feature들은 레이블이 일부만 있는 데이터를 사용할 수 있을때, D의 성능을 향상시킬 수 있다.
- 효율성 향상 : G와 D를 조정하는 더 나은 방법을 설명하거나 학습 중에 z 샘플에 더 나은 분포를 결정함으로써 학습의 속도를 높일 수 있다.